{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "(x_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = (x_train.astype('float32') - 127.5) / 127.5\n",
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 64\n",
        "LATENT_DIM = 100\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "RJCTEnnj4KEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Dense(8*8*256, use_bias=False, input_shape=(LATENT_DIM,)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Reshape((8, 8, 256)),\n",
        "\n",
        "        layers.Conv2DTranspose(128, (5,5), strides=(2,2), padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Conv2DTranspose(3, (5,5), strides=(1,1), padding='same', use_bias=False, activation='tanh')\n",
        "    ])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "LNQLa-TV4KBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=[32, 32, 3]),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Conv2D(128, (5,5), strides=(2,2), padding='same'),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "YLHKjJYv4J_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n"
      ],
      "metadata": {
        "id": "3v0gAJm74J9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "disc_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss"
      ],
      "metadata": {
        "id": "WhNSKS1I4J5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, LATENT_DIM])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n"
      ],
      "metadata": {
        "id": "feWb4DLP4J22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(epoch):\n",
        "    noise = tf.random.normal([16, LATENT_DIM])\n",
        "    generated_images = generator(noise, training=False)\n",
        "\n",
        "    fig, axs = plt.subplots(4, 4, figsize=(5, 5))\n",
        "    for i, ax in enumerate(axs.flat):\n",
        "        img = (generated_images[i] + 1) / 2\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.savefig(f'generated_epoch_{epoch}.png')\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "Rpe7b38f4Jy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for image_batch in train_dataset:\n",
        "            train_step(image_batch)\n",
        "\n",
        "        generate_and_save_images(epoch)\n",
        "        print(f\"Epoch {epoch+1} completed.\")\n",
        "\n",
        "last_image = train(epochs=70)\n",
        "\n",
        "\n",
        "if last_image:\n",
        "    final_image = plt.imread(last_image)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(final_image)\n",
        "    plt.title(\"Final Generated Image\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVFNd1Kfl9fC",
        "outputId": "b4b437a8-d8dd-4398-8aa8-250903827206"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed.\n",
            "Epoch 2 completed.\n",
            "Epoch 3 completed.\n",
            "Epoch 4 completed.\n",
            "Epoch 5 completed.\n",
            "Epoch 6 completed.\n",
            "Epoch 7 completed.\n",
            "Epoch 8 completed.\n",
            "Epoch 9 completed.\n",
            "Epoch 10 completed.\n",
            "Epoch 11 completed.\n",
            "Epoch 12 completed.\n",
            "Epoch 13 completed.\n",
            "Epoch 14 completed.\n",
            "Epoch 15 completed.\n",
            "Epoch 16 completed.\n",
            "Epoch 17 completed.\n",
            "Epoch 18 completed.\n",
            "Epoch 19 completed.\n",
            "Epoch 20 completed.\n",
            "Epoch 21 completed.\n",
            "Epoch 22 completed.\n",
            "Epoch 23 completed.\n",
            "Epoch 24 completed.\n",
            "Epoch 25 completed.\n",
            "Epoch 26 completed.\n",
            "Epoch 27 completed.\n",
            "Epoch 28 completed.\n",
            "Epoch 29 completed.\n",
            "Epoch 30 completed.\n",
            "Epoch 31 completed.\n",
            "Epoch 32 completed.\n",
            "Epoch 33 completed.\n",
            "Epoch 34 completed.\n",
            "Epoch 35 completed.\n",
            "Epoch 36 completed.\n",
            "Epoch 37 completed.\n",
            "Epoch 38 completed.\n",
            "Epoch 39 completed.\n",
            "Epoch 40 completed.\n",
            "Epoch 41 completed.\n",
            "Epoch 42 completed.\n",
            "Epoch 43 completed.\n",
            "Epoch 44 completed.\n",
            "Epoch 45 completed.\n",
            "Epoch 46 completed.\n",
            "Epoch 47 completed.\n",
            "Epoch 48 completed.\n",
            "Epoch 49 completed.\n",
            "Epoch 50 completed.\n",
            "Epoch 51 completed.\n",
            "Epoch 52 completed.\n",
            "Epoch 53 completed.\n",
            "Epoch 54 completed.\n",
            "Epoch 55 completed.\n",
            "Epoch 56 completed.\n",
            "Epoch 57 completed.\n",
            "Epoch 58 completed.\n",
            "Epoch 59 completed.\n",
            "Epoch 60 completed.\n",
            "Epoch 61 completed.\n",
            "Epoch 62 completed.\n",
            "Epoch 63 completed.\n",
            "Epoch 64 completed.\n",
            "Epoch 65 completed.\n",
            "Epoch 66 completed.\n",
            "Epoch 67 completed.\n",
            "Epoch 68 completed.\n",
            "Epoch 69 completed.\n",
            "Epoch 70 completed.\n"
          ]
        }
      ]
    }
  ]
}